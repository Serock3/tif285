<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Learning from data: Gaussian processes">

<title>Learning from data: Gaussian processes</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 2,
 'sections': [('Inference using Gaussian processes', 2, None, '___sec0'),
              ('References:', 3, None, '___sec1'),
              ('Parametric approach', 2, None, '___sec2'),
              ('Proof', 3, None, '___sec3'),
              ('The covariance matrix as the central object',
               3,
               None,
               '___sec4'),
              ('Non-parametric approach: Mean and covariance functions',
               2,
               None,
               '___sec5'),
              ('Stationary kernels', 3, None, '___sec6'),
              ('GP models for regression', 2, None, '___sec7'),
              ('Elegant approach using linear algebra tricks',
               3,
               None,
               '___sec8'),
              ('Optimizing the GP model hyperparameters', 3, None, '___sec9')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="GaussianProcesses-bs.html">Learning from data: Gaussian processes</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="#___sec0" style="font-size: 80%;"><b>Inference using Gaussian processes</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec1" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;References:</a></li>
     <!-- navigation toc: --> <li><a href="#___sec2" style="font-size: 80%;"><b>Parametric approach</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec3" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Proof</a></li>
     <!-- navigation toc: --> <li><a href="#___sec4" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;The covariance matrix as the central object</a></li>
     <!-- navigation toc: --> <li><a href="#___sec5" style="font-size: 80%;"><b>Non-parametric approach: Mean and covariance functions</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec6" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Stationary kernels</a></li>
     <!-- navigation toc: --> <li><a href="#___sec7" style="font-size: 80%;"><b>GP models for regression</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec8" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Elegant approach using linear algebra tricks</a></li>
     <!-- navigation toc: --> <li><a href="#___sec9" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Optimizing the GP model hyperparameters</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<!-- ------------------- main content ---------------------- -->



<div class="jumbotron">
<center><h1>Learning from data: Gaussian processes</h1></center>  <!-- document title -->

<p>
<!-- author(s): Christian Forss&#233;n -->

<center>
<b>Christian Forss&#233;n</b> 
</center>

<p>
<!-- institution -->

<center><b>Department of Physics, Chalmers University of Technology, Sweden</b></center>
<br>
<p>
<center><h4>Oct 21, 2019</h4></center> <!-- date -->
<br>
<p>
<!-- potential-jumbotron-button -->
</div> <!-- end jumbotron -->

<!-- !split -->

<h2 id="___sec0" class="anchor">Inference using Gaussian processes </h2>

<p>
Assume that there is a set of input vectors with independent, predictor, variables
$$ \boldsymbol{X}_N \equiv \{ \boldsymbol{x}^{(n)}\}_{n=1}^N $$

and a set of target values
$$ \boldsymbol{t}_N \equiv \{ t^{(n)}\}_{n=1}^N. $$


<ul>
<li> Note that we will use the symbol \( t \) to denote the target, or response, variables in the context of Gaussian Processes.</li> 
<li> Furthermore, we will use the subscript \( N \) to denote a vector of \( N \) vectors (or scalars): \( \boldsymbol{X}_N \) (\( \boldsymbol{t}_N \))</li>
<li> While a single instance \( i \) is denoted by a superscript: \( \boldsymbol{x}^{(i)} \) (\( t^{(i)} \)).</li>
</ul>

<!-- !split -->
We will consider two different <em>inference problems</em>:

<ol>
<li> The prediction of <em>new target</em> \( t^{(N+1)} \) given a new input \( \boldsymbol{x}^{(N+1)} \)</li>
<li> The inference of a <em>function</em> \( y(\boldsymbol{x}) \) from the data.</li>
</ol>

<!-- !split -->
The former can be expressed with the pdf
$$ 
p\left( t^{(N+1)} | \boldsymbol{t}_N, \boldsymbol{X}_{N}, \boldsymbol{x}^{(N+1)} \right)
$$

while the latter can be written using Bayes' formula (in these notes we will not be including information \( I \) explicitly in the conditional probabilities)
$$ p\left( y(\boldsymbol{x}) | \boldsymbol{t}_N, \boldsymbol{X}_N \right)
= \frac{p\left( \boldsymbol{t}_N | y(\boldsymbol{x}), \boldsymbol{X}_N \right) p \left( y(\boldsymbol{x}) \right) }
{p\left( \boldsymbol{t}_N | \boldsymbol{X}_N \right) } $$

<p>
<!-- !split -->
The inference of a function will obviously also allow to make predictions for new targets. 
However, we will need to consider in particular the second term in the numerator, which is the <b>prior</b> distribution on functions assumed in the model.

<ul>
<li> This prior is implicit in parametric models with priors on the parameters.</li>
<li> The idea of Gaussian process modeling is to put a prior directly on the <b>space of functions</b> without parameterizing \( y(\boldsymbol{x}) \).</li>
<li> A Gaussian process can be thought of as a generalization of a Gaussian distribution over a finite vector space to a <b>function space of infinite dimension</b>.</li>
<li> Just as a Gaussian distribution is specified by its mean and covariance matrix, a Gaussian process is specified by a <b>mean and covariance function</b>.</li>
</ul>

<!-- !split -->
<div class="panel panel-primary">
  <div class="panel-heading">
  <h3 class="panel-title">Gaussian process</h3>
  </div>
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
A Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution
</div>
</div>


<p>
<!-- !split -->

<h3 id="___sec1" class="anchor">References: </h3>

<ol>
<li> <a href="http://www.gaussianprocess.org/gpml" target="_self">Gaussian Processes for Machine Learning</a>, Carl Edward Rasmussen and Chris Williams, the MIT Press, 2006, <a href="http://www.gaussianprocess.org/gpml/chapters" target="_self">online version</a>.</li>
<li> <a href="https://sheffieldml.github.io/GPy/" target="_self">GPy</a>: a Gaussian Process (GP) framework written in python, from the Sheffield machine learning group.</li>
</ol>

<!-- !split -->

<h2 id="___sec2" class="anchor">Parametric approach </h2>

<p>
Let us express \( y(\boldsymbol{x}) \) in terms of a model function \( y(\boldsymbol{x}; \boldsymbol{\theta}) \) that depends on a vector of model parameters \( \boldsymbol{\theta} \).

<p>
For example, using a set of basis functions \( \left\{ \phi^{(h)} (\boldsymbol{x}) \right\}_{h=1}^H \) with linear weights \( \boldsymbol{\theta}_H \) we have
$$
y (\boldsymbol{x}, \boldsymbol{\theta}) = \sum_{h=1}^H \theta^{(h)} \phi^{(h)} (\boldsymbol{x})
$$

<p>
<div class="panel panel-primary">
  <div class="panel-heading">
  <h3 class="panel-title">Notice</h3>
  </div>
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
The basis functions can be non-linear such as Gaussians (aka <em>radial basis functions</em>)
$$
\phi^{(h)} (\boldsymbol{x}) = \exp \left[ -\frac{\left( \boldsymbol{x} - \boldsymbol{c}^{(h)} \right)^2}{2 (\sigma^{(h)})^2} \right].
$$

<p>
Still, this constitutes a linear model since \( y (\boldsymbol{x}, \boldsymbol{\theta}) \) depends linearly on the parameters \( \boldsymbol{\theta} \).
</div>
</div>


<p>
The inference of model parameters should be a well-known problem by now. We state it in terms of Bayes theorem
$$
p \left( \boldsymbol{\theta} | \boldsymbol{t}_N, \boldsymbol{X}_N \right)
= \frac{ p \left( \boldsymbol{t}_N | \boldsymbol{\theta}, \boldsymbol{X}_N \right) p \left( \boldsymbol{\theta} \right)}{p \left( \boldsymbol{t}_N | \boldsymbol{X}_N \right)}
$$

<p>
Having solved this inference problem (e.g. by linear regression) a prediction can be made through marginalization
$$
p\left( t^{(N+1)} | \boldsymbol{t}_N, \boldsymbol{X}_{N}, \boldsymbol{x}^{(N+1)} \right) 
= \int d^H \boldsymbol{\theta} 
p\left( t^{(N+1)} | \boldsymbol{\theta}, \boldsymbol{x}^{(N+1)} \right)
p \left( \boldsymbol{\theta} | \boldsymbol{t}_N, \boldsymbol{X}_N \right).
$$

Here it is important to note that the final answer does not make any explicit reference to our parametric representation of the unknown function \( y(\boldsymbol{x}) \).

<p>
Assuming that we have a fixed set of basis functions and Gaussian prior distributions (with zero mean) on the weights \( \boldsymbol{\theta} \) we will show that:

<ul>
<li> The joint pdf of the observed data given the model \( p( \boldsymbol{t}_N |  \boldsymbol{X}_N) \), is a multivariate Gaussian with mean zero and with a covariance matrix that is determined by the basis functions.</li>
<li> This implies that the conditional distribution \( p( t^{(N+1)} | \boldsymbol{t}_N, \boldsymbol{X}_{N+1}) \), is also a multivariate Gaussian whose mean depends linearly on \( \boldsymbol{t}_N \).</li>
</ul>

<h3 id="___sec3" class="anchor">Proof </h3>

<p>
<div class="panel panel-primary">
  <div class="panel-heading">
  <h3 class="panel-title">Sum of normally distributed random variables</h3>
  </div>
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
If \( X \) and \( Y \) are independent random variables that are normally distributed (and therefore also jointly so), then their sum is also normally distributed. i.e., \( Z=X+Y \) is normally distributed with its mean being the sum of the two means, and its variance being the sum of the two variances.
</div>
</div>


<p>
Consider the linear model and define the \( N \times H \) design matrix \( \boldsymbol{R} \) with elements
$$
R_{nh} \equiv \phi^{(h)} \left( \boldsymbol{x}^{(n)} \right).
$$

<p>
Then \( \boldsymbol{y}_N = \boldsymbol{R} \boldsymbol{\theta} \) is the vector of model predictions, i.e.
$$
y^{(n)} = \sum_{h=1}^H R_{nh} \boldsymbol{\theta^{(h)}}.
$$

<p>
Assume that we have a Gaussian prior for the linear model weights \( \boldsymbol{\theta} \) with zero mean and a diagonal covariance matrix
$$
p(\boldsymbol{\theta}) = \mathcal{N} \left( \boldsymbol{\theta}; 0, \sigma_\theta^2 \boldsymbol{I} \right).
$$

<p>
Now, since \( y \) is a linear function of \( \boldsymbol{\theta} \), it is also Gaussian distributed with mean zero. Its covariance matrix becomes
$$
\boldsymbol{Q} = \langle \boldsymbol{y} \boldsymbol{y}^T \rangle = \langle \boldsymbol{R} \boldsymbol{\theta} \boldsymbol{\theta}^T \boldsymbol{R}^T \rangle
= \sigma_\theta^2 \boldsymbol{R} \boldsymbol{R}^T,
$$

which implies that
$$
p(\boldsymbol{y}) = \mathcal{N} \left( \boldsymbol{y}; 0, \sigma_\theta^2 \boldsymbol{R} \boldsymbol{R}^T \right).
$$

This will be true for any set of points \( \boldsymbol{X}_N \); which is the defining property of a <b>Gaussian process</b>.

<ul>
<li> What about the target values \( \boldsymbol{t} \)?</li>
</ul>

Well, if \( t^{(n)} \) is assumed to differ by additive Gaussian noise, i.e., 
$$
t^{(n)} = y^{(n)} + \varepsilon^{(n)}, 
$$

where \( \varepsilon^{(n)} \sim \mathcal{N} \left( 0, \sigma_\nu^2 \right) \); then \( \boldsymbol{t} \) also has a Gaussian prior distribution
$$
p(\boldsymbol{t}) = \mathcal{N} \left( \boldsymbol{t}; 0, \boldsymbol{C} \right),
$$

where the covariance matrix of this target distribution is given by
$$
\boldsymbol{C} = \boldsymbol{Q} + \sigma_\nu^2 \boldsymbol{I} = \sigma_\theta^2 \boldsymbol{R} \boldsymbol{R}^T + \sigma_\nu^2 \boldsymbol{I}.
$$

<p>
<!-- !split -->

<h3 id="___sec4" class="anchor">The covariance matrix as the central object </h3>

The covariance matrices are given by
$$
Q_{nn'} = \sigma_\theta^2 \sum_h \phi^{(h)} \left( \boldsymbol{x}^{(n)} \right) \phi^{(h)} \left( \boldsymbol{x}^{(n')} \right),
$$

and
$$
C_{nn'} = Q_{nn'} + \delta_{nn'} \sigma_\nu^2.
$$

This means that the correlation between target values \( t^{(n)} \) and \( t^{(n')} \) is determined by the points \( \boldsymbol{x}^{(n)} \), \( \boldsymbol{x}^{(n')} \) and the behaviour of the basis functions.

<p>
<!-- !split -->

<h2 id="___sec5" class="anchor">Non-parametric approach: Mean and covariance functions </h2>

<p>
In fact, we don't really need the basis functions and their parameters anymore. The influence of these appear only in the covariance matrix that describes the distribution of the targets, which is our key object. We can replace the parametric model altogether with a <b>covariance function</b> \( C( \boldsymbol{x}, \boldsymbol{x}' ) \) which generates the  elements of the covariance matrix
$$
Q_{nn'} = C \left( \boldsymbol{x}^{(n)}, \boldsymbol{x}^{(n')} \right),
$$

for any set of points \( \left\{ \boldsymbol{x}^{(n)} \right\}_{n=1}^N \).

<p>
Note, however, that \( \boldsymbol{Q} \) must be positive-definite. This constrains the set of valid covariance functions.

<p>
Once we have defined a covariance function, the covariance matrix for the target values will be given by
$$
C_{nn'} = C \left( \boldsymbol{x}^{(n)}, \boldsymbol{x}^{(n')} \right) + \sigma_\nu^2 \delta_{nn'}.
$$

<p>
A wide range of different covariance contributions can be <a href="https://en.wikipedia.org/wiki/Gaussian_process#Covariance_functions" target="_self">constructed</a>. These standard covariance functions are typically parametrized with hyperparameters \( \boldsymbol{\alpha} \) so that 
$$
C_{nn'} = C \left( \boldsymbol{x}^{(n)}, \boldsymbol{x}^{(n')}, \boldsymbol{\alpha} \right) + \delta_{nn'} \Delta \left( \boldsymbol{x}^{(n)};  \boldsymbol{\alpha} \right),
$$

where \( \Delta \) is usually included as a flexible noise model.

<p>
<!-- !split -->

<h3 id="___sec6" class="anchor">Stationary kernels </h3>

The most common types of covariance functions are stationary, or translationally invariant, which implies that 
$$
C \left( \boldsymbol{x}^{(n)}, \boldsymbol{x}^{(n')}, \boldsymbol{\alpha} \right) = D \left( \boldsymbol{x} - \boldsymbol{x}'; \boldsymbol{\alpha} \right),
$$

where the function \( D \) is often referred to as a <em>kernel</em>.

<p>
A very standard kernel is the RBF (also known as Exponentiated Quadratic or Gaussian kernel) which is differentiable infinitely many times (hence, very smooth),
$$ 
C_\mathrm{RBF}(\mathbf{x},\mathbf{x}'; \boldsymbol{\alpha}) = \alpha_0 + \alpha_1 \exp \left[ -\frac{1}{2} \sum_{i=1}^I \frac{(x_{i} - x_{i}')^2}{r_i^2} \right] 
$$

where \( I \) denotes the dimensionality of the input space. The hyperparameters are: \( \{ \alpha_0, \alpha_1, \vec{r} \} \). Sometimes, a single correlation length \( r \) is used.

<p>
<!-- !split -->

<h2 id="___sec7" class="anchor">GP models for regression </h2>
Let us return to the problem of predicting \( t^{(N+1)} \) given \( \boldsymbol{t}_N \). The independent variables \( \boldsymbol{X}_{N+1} \) are also given, but will be omitted from the conditional pdfs below.

<p>
The joint density is
$$
p \left( t^{(N+1)}, \boldsymbol{t}_N \right) = p \left( t^{(N+1)} | \boldsymbol{t}_N \right) p \left( \boldsymbol{t}_N \right) 
\quad \Rightarrow \quad
p \left( t^{(N+1)} | \boldsymbol{t}_N \right) = \frac{p \left( t^{(N+1)}, \boldsymbol{t}_N \right)}{p \left( \boldsymbol{t}_N \right) }.
$$

<p>
Since both \( p \left( t^{(N+1)}, \boldsymbol{t}_N \right) \) and \( p \left( \boldsymbol{t}_N \right) \) are Gaussian distributions, then the conditional distribution, obtained by the ratio, must also be a Gaussian. Let us use the notation \( \boldsymbol{C}_{N+1} \) for the \( (N+1) \times (N+1) \) covariance matrix for \( \boldsymbol{t}_{N+1} = \left( \boldsymbol{t}_N, t^{(N+1)} \right) \). This implies that
$$
p \left( t^{(N+1)} | \boldsymbol{t}_N \right) \propto \exp \left[ -\frac{1}{2} \left( \boldsymbol{t}_N, t^{(N+1)} \right) \boldsymbol{C}_{N+1}^{-1} 
\begin{pmatrix}
\boldsymbol{t}_N \\
t^{(N+1)}
\end{pmatrix}
\right]
$$

<p>
<div class="panel panel-danger">
  <div class="panel-heading">
  <h3 class="panel-title">Summary</h3>
  </div>
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
The prediction of the (Gaussian) pdf for \( t^{(N+1)} \) requires an inversion of the covariance matrix \( \boldsymbol{C}_{N+1}^{-1} \).
</div>
</div>


<h3 id="___sec8" class="anchor">Elegant approach using linear algebra tricks </h3>

Let us split the \( \boldsymbol{C}_{N+1} \) covariance matrix into four different blocks
$$
\boldsymbol{C}_{N+1} =
\begin{pmatrix}
\boldsymbol{C}_N & \boldsymbol{k} \\
\boldsymbol{k}^T & \kappa
\end{pmatrix},
$$

where \( \boldsymbol{C}_N \) is the \( N \times N \) covariance matrix (which depends on the positions \( \boldsymbol{X}_N \)), \( \boldsymbol{k} \) is an \( N \times 1 \) vector (that describes the covariance of \( \boldsymbol{X}_N \) with \( \boldsymbol{x}^{(N+1)} \)), while \( \kappa \) is the single diagonal element obtained from \( \boldsymbol{x}^{(N+1)} \).

<p>
We can use the partitioned inverse equations (Barnett, 1979) to rewrite \( \boldsymbol{C}_{N+1}^{-1} \) in terms of \( \boldsymbol{C}_{N}^{-1} \) and \( \boldsymbol{C}_{N} \) as follows
$$
\boldsymbol{C}_{N+1}^{-1} =
\begin{pmatrix}
\boldsymbol{M}_N & \boldsymbol{m} \\
\boldsymbol{m}^T & \mu
\end{pmatrix},
$$

where
$$
\begin{align*}
\mu &= \left( \kappa - \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{k} \right)^{-1} \\
\boldsymbol{m} &= -\mu \boldsymbol{C}_N^{-1} \boldsymbol{k} \\
\boldsymbol{M}_N &= \boldsymbol{C}_N^{-1} + \frac{1}{\mu} \boldsymbol{m} \boldsymbol{m}^T.
\end{align*}
$$

<p>
<div class="panel panel-success">
  <div class="panel-heading">
  <h3 class="panel-title">Question</h3>
  </div>
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
What are the dimensions of the different blocks? Check that the answer.
</div>
</div>


<p>
This implies that we can make a prediction for the Gaussian pdf of \( t^{(N+1)} \) (meaning that we predict its value with an associated uncertainty) for an \( N^3 \) computational cost (the inversion of an \( N \times N \) matrix).

<p>
<div class="panel panel-danger">
  <div class="panel-heading">
  <h3 class="panel-title">Summary</h3>
  </div>
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
The prediction for \( t^{(N+1)} \) is a Gaussian
$$
p \left( t^{(N+1)} | \boldsymbol{t}_N \right) = \frac{1}{Z} \exp
\left[
-\frac{\left( t^{(N+1)} - \hat{t}^{(N+1)} \right)^2}{2 \sigma_{\hat{t}_{N+1}}^2}
\right]
$$

with
$$
\begin{align*}
\mathrm{mean:} & \quad \hat{t}^{(N+1)} = \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{t}_N \\
\mathrm{variance:} & \quad \sigma_{\hat{t}_{N+1}}^2 = \kappa - \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{k}.
\end{align*}
$$
</div>
</div>


<p>
In fact, since the prediction only depends on the \( N \) available data we might as well predict several new target values at once. Consider \( \boldsymbol{t}_M = \{ t^{(N+i)} \}_{i=1}^M \) so that
$$
\boldsymbol{C}_{N+M} =
\begin{pmatrix}
\boldsymbol{C}_N & \boldsymbol{k} \\
\boldsymbol{k}^T & \boldsymbol{\kappa}
\end{pmatrix},
$$

where \( \boldsymbol{k} \) is now an \( N \times M \) matrix and \( \boldsymbol{\kappa} \) an \( M \times M \) matrix.

<p>
The prediction becomes a multivariate Gaussian
$$
p \left( \boldsymbol{t}_{N+M} | \boldsymbol{t}_N \right) = \frac{1}{Z} \exp
\left[
-\frac{1}{2} \left( \boldsymbol{t}_M - \hat{\boldsymbol{t}}_M \right)^T \boldsymbol{\Sigma}_M^{-1} \left( \boldsymbol{t}_M - \hat{\boldsymbol{t}}_M \right)
\right],
$$

where the \( M \times 1 \) mean vector and \( M \times M \) covariance matrix are
$$
\begin{align*}
\hat{\boldsymbol{t}}_M &= \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{t}_N \\
\boldsymbol{\Sigma}_M &= \boldsymbol{\kappa} - \boldsymbol{k}^T \boldsymbol{C}_N^{-1} \boldsymbol{k}.
\end{align*}
$$

<h3 id="___sec9" class="anchor">Optimizing the GP model hyperparameters </h3>

Predictions can be made once we have

<ol>
<li> Chosen an appropriate covariance function.</li>
<li> Determined the hyperparameters.</li>
<li> Evaluated the relevant blocks in the covariance function and inverted \( \\boldsymbol{C}_N \).</li>
</ol>

How do we determine the hyperparameters \( \boldsymbol{\alpha} \)? Well, recall that
$$
p \left( \boldsymbol{t}_N \right) = \frac{1}{Z} \exp \left[ -\frac{1}{2} \boldsymbol{t}_N^T \boldsymbol{C}_{N}^{-1} \boldsymbol{t}_N 
\right].
$$

This pdf is basically a data likelihood.

<ul>
<li> The frequentist approach would be to find the set of hyperparameters \( \boldsymbol{\alpha}^* \) that maximizes the data likelihood, i.e. that minimizes \( \boldsymbol{t}_N^T \boldsymbol{C}_{N}^{-1} \boldsymbol{t}_N \).</li>
<li> A Bayesian approach would be to assign a prior to the hyperparameters and seek a posterior pdf \( p(\boldsymbol{\alpha} | \boldsymbol{t}_N) \) instead.</li>
</ul>

The former approach is absolutely dominating the literature on GP regression. The covariance function hyperparameters are first optimized and then used for regression.

<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright --> &copy; 2018-2019, Christian Forss&#233;n. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    

