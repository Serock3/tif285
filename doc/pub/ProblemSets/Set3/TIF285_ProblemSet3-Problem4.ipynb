{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3\n",
    "## Learning from data [TIF285], Chalmers, Fall 2019\n",
    "\n",
    "Last revised: 17-Oct-2019 by Christian Forssén [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See deadline on the course web page\n",
    "- This problem set is performed individually (but collaboration is encouraged) and contains a number of basic and extra problems; you can choose which and how many to work on.\n",
    "- See examination rules on the course web page.\n",
    "- Hand-in is performed through the following **two** actions:\n",
    "  - Upload of your solution in the form of a jupyter notebook, or python code, via Canvas.\n",
    "  - Answer the corresponding questions on OpenTA.\n",
    "  \n",
    "  Note that the hand-in is not complete, and will not be graded, if any of those actions is not performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill your personal details\n",
    "- Name: **Lastname, Firstname**\n",
    "- Personnummer: **YYMMDD-XXXX**\n",
    "  <br/>\n",
    "  (civic registration number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems\n",
    "The 10 basic points of this problem set are distributed over four problems:\n",
    "1. Assigning probabilities for a hundred-sided die (2 basic points)\n",
    "2. Chi-squared hypothesis testing (2 basic points)\n",
    "3. Gaussian process regression (3 basic points)\n",
    "4. Neural network classification (3 basic points)\n",
    "\n",
    "Each of them will be presented in a separate jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Neural network classification\n",
    "### (3 basic points)\n",
    "<!-- Author: -->  \n",
    "Christian Forssén, Chalmers, and\n",
    "Morten Hjorth-Jensen, University of Oslo and Michigan State University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to implement a fully-connected neural network with a single hidden layer (50 nodes) and a 10-node output layer that works as a multi-label classifier. Specifically, the neural network should be used to classify images of hand written digits using a reduced version of the MNIST database.\n",
    "* In particular, you need to implement the forward pass.\n",
    "* A working code for back-propagation is included, but you will have to combine the different ingredients and perform the training.\n",
    "* Finally, you should study some properties of the final network and some details of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific tasks are described at the various stages of the neural network implementation which is outlined below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a code for doing neural networks with back propagation\n",
    "\n",
    "One can identify a set of key steps when using neural networks to solve supervised learning problems:  \n",
    "  \n",
    "1. Collect and pre-process data  \n",
    "1. Define model and architecture  \n",
    "1. Choose cost function and optimizer  \n",
    "1. Train the model  \n",
    "1. Adjust hyperparameters (if necessary, network architecture)\n",
    "1. Evaluate model performance on test data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect and pre-process data\n",
    "\n",
    "Here we will be using the MNIST dataset, which is readily available through the **scikit-learn**\n",
    "package. You may also find it for example [here](http://yann.lecun.com/exdb/mnist/).  \n",
    "The *MNIST* (Modified National Institute of Standards and Technology) database is a large database\n",
    "of handwritten digits that is commonly used for training various image processing systems.  \n",
    "The MNIST dataset consists of 70 000 images of size $28\\times 28$ pixels, each labeled from 0 to 9.  \n",
    "The scikit-learn dataset we will use consists of a selection of 1797 images of size $8\\times 8$ collected and processed from this database.  \n",
    "\n",
    "Each input image is a 2D matrix, but we will flatten the image\n",
    "(i.e. \"unravel\" the 2D matrix into a 1D array) to turn the data into a\n",
    "design/feature matrix. This means we lose all spatial information in the\n",
    "image, such as locality and translational invariance. More complicated\n",
    "architectures such as Convolutional Neural Networks can take advantage\n",
    "of such information, and are most commonly applied when analyzing\n",
    "images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (n_data, pixel_width, pixel_height) = (1797, 8, 8)\n",
      "                       with labels (n_data) = (1797,)\n",
      "\n",
      "flattened input, X: (n_inputs, n_features)  = (1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import \n",
    "from sklearn import datasets\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# download MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# define inputs and labels\n",
    "inputs = digits.images\n",
    "labels = digits.target\n",
    "\n",
    "print(f\"inputs: (n_data, pixel_width, pixel_height) = {inputs.shape}\")\n",
    "print(f\"                       with labels (n_data) = {labels.shape}\")\n",
    "\n",
    "\n",
    "# flatten the image\n",
    "# the value -1 means dimension is inferred from the remaining dimensions: 8x8 = 64\n",
    "n_inputs = len(inputs)\n",
    "inputs = inputs.reshape(n_inputs, -1)\n",
    "print(f\"\\nflattened input, X: (n_inputs, n_features)  = {inputs.shape}\")\n",
    "\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(n_inputs)\n",
    "random_indices = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[random_indices]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(f\"Label: {digits.target[random_indices[i]]:1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test datasets\n",
    "\n",
    "Performing analysis before partitioning the dataset is a major error, that can lead to incorrect conclusions.  \n",
    "\n",
    "We will reserve $70 \\%$ of our dataset for training and $30 \\%$ for testing.  \n",
    "\n",
    "It is important that the train and test datasets are drawn randomly from our dataset, to ensure\n",
    "no bias in the sampling.  \n",
    "Say you are taking measurements of weather data to predict the weather in the coming 5 days.\n",
    "You don't want to train your model on measurements taken from the hours 00.00 to 12.00, and then test it on data\n",
    "collected from 12.00 to 24.00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 1257\n",
      "Number of test images:      540\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "train_size = 0.7\n",
    "test_size = 1 - train_size\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inputs, labels, train_size=train_size,\n",
    "                                                    test_size=test_size)\n",
    "\n",
    "print(f\"Number of training images: {len(X_train):4}\")\n",
    "print(f\"Number of test images:     {len(X_test):4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model and architecture\n",
    "\n",
    "Our simple feed-forward neural network will consist of an *input* layer, a single *hidden* layer and an *output* layer. The activation $y$ of each neuron is a weighted sum of inputs, passed through an activation function. In case of the simple perceptron model we have \n",
    "\n",
    "$$ z = \\sum_{i=1}^n w_i a_i + b,$$\n",
    "\n",
    "$$ y = f(z) ,$$\n",
    "\n",
    "where $f$ is the activation function, $a_i$ represents input from neuron $i$ in the preceding layer,\n",
    "$w_i$ is the weight to input $i$ and $b$ is the bias.\n",
    "\n",
    "The activation of the neurons in the input layer is just the features (e.g. a pixel value).  \n",
    "\n",
    "The simplest activation function for a neuron is the *Heaviside* step function. Other typical choices for activation functions include the sigmoid function, hyperbolic tangent, and Rectified Linear Unit (ReLU).  \n",
    "We will be using the sigmoid function $\\sigma(z)$:  \n",
    "\n",
    "$$ f(z) = \\sigma(z) = \\frac{1}{1 + e^{-z}} ,$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers\n",
    "\n",
    "* Input \n",
    "\n",
    "Since each input image has 8x8 = 64 pixels or features, we have an input layer of 64 neurons.  \n",
    "\n",
    "* Hidden layer\n",
    "\n",
    "We will use 50 neurons in the hidden layer receiving input from the neurons in the input layer.  \n",
    "Since each neuron in the hidden layer is connected to the 64 inputs we have 64x50 = 3200 weights to the hidden layer.  We also have 50 biases.\n",
    "\n",
    "* Output\n",
    "\n",
    "If we were building a binary classifier, it would be sufficient with a single neuron in the output layer,\n",
    "which could output 0 or 1 according to the Heaviside function. This would be an example of a *hard* classifier, meaning it outputs the class of the input directly. However, if we are dealing with noisy data it is often beneficial to use a *soft* classifier, which outputs the probability of being in class 0 or 1.  \n",
    "\n",
    "For a soft binary classifier, we could use a single neuron and interpret the output as either being the probability of being in class 0 or the probability of being in class 1. Alternatively we could use 2 neurons, and interpret each neuron as the probability of being in each class.  \n",
    "\n",
    "Since we are doing multiclass classification, with 10 categories, it is natural to use 10 neurons in the output layer. We number the neurons $j = 0,1,...,9$. The activation of each output neuron $j$ will be according to the *softmax* function:  \n",
    "\n",
    "$$ P(\\text{class $j$} \\mid \\text{input $\\hat{a}$}) = \\frac{\\exp{(\\hat{a}^T \\hat{w}_j)}}\n",
    "{\\sum_{c=0}^{9} \\exp{(\\hat{a}^T \\hat{w}_c)}} ,$$  \n",
    "\n",
    "i.e. each neuron $j$ outputs the probability of being in class $j$ given an input from the hidden layer $\\hat{a}$, with $\\hat{w}_j$ the weights of neuron $j$ to the inputs.  \n",
    "The denominator is a normalization factor to ensure the outputs (probabilities) sum up to 1.  \n",
    "The exponent is just the weighted sum of inputs as before:  \n",
    "\n",
    "$$ z_j = \\sum_{i=1}^n w_ {ij} a_i+b_j.$$  \n",
    "\n",
    "Since each neuron in the output layer is connected to the 50 inputs from the hidden layer we have 50x10 = 500\n",
    "weights to the output layer. We also have 10 biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and biases\n",
    "\n",
    "Typically weights are initialized with small values distributed around zero, drawn from a uniform\n",
    "or normal distribution. Setting all weights to zero means all neurons give the same output, making the network useless.  \n",
    "\n",
    "Adding a bias value to the weighted sum of inputs allows the neural network to represent a greater range\n",
    "of values. Without it, any input with the value 0 will be mapped to zero (before being passed through the activation).  \n",
    "\n",
    "$$ z_j = \\sum_{i=1}^n w_ {ij} a_i + b_j.$$  \n",
    "\n",
    "The bias weights $\\hat{b}$ are often initialized to zero, but a small value like $0.01$ ensures all neurons have some output which can be backpropagated in the first training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our neural network\n",
    "\n",
    "n_inputs, n_features = X_train.shape\n",
    "n_hidden_neurons = 50\n",
    "n_categories = 10\n",
    "\n",
    "# we make the weights normally distributed using numpy.random.randn\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# weights and bias in the hidden layer\n",
    "hidden_weights = np.random.randn(n_features, n_hidden_neurons)\n",
    "hidden_bias = np.zeros(n_hidden_neurons) + 0.01\n",
    "\n",
    "# weights and bias in the output layer\n",
    "output_weights = np.random.randn(n_hidden_neurons, n_categories)\n",
    "output_bias = np.zeros(n_categories) + 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward pass\n",
    "\n",
    "Denote $F$ the number of features (inputs), $H$ the number of hidden neurons and $C$ the number of categories.  \n",
    "For each input image we calculate a weighted sum of input features (pixel values) to each neuron $j$ in the hidden layer $h$:  \n",
    "\n",
    "$$ z_{j}^{l} = \\sum_{i=1}^{F} w_{ij}^{h} x_i + b_{j}^{h},$$\n",
    "\n",
    "this is then passed through our activation function  \n",
    "\n",
    "$$ a_{j}^{h} = f(z_{j}^{h}) .$$  \n",
    "\n",
    "We calculate a weighted sum of inputs (activations in the hidden layer) to each neuron $j$ in the output layer:  \n",
    "\n",
    "$$ z_{j}^{L} = \\sum_{i=1}^{H} w_{ij}^{L} a_{i}^{h} + b_{j}^{L}.$$  \n",
    "\n",
    "Finally we calculate the output of neuron $j$ in the output layer using the softmax function:  \n",
    "\n",
    "$$ y_{j}^{L} = \\frac{\\exp{(z_j^{L})}}\n",
    "{\\sum_{c=0}^{C-1} \\exp{(z_c^{L})}} .$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplications\n",
    "\n",
    "Since our data has the dimensions $X = (n_\\mathrm{data}, n_\\mathrm{features})$ and our weights [biases] to the hidden layer have the dimensions  \n",
    "$W^\\mathrm{h} = (n_\\mathrm{features}, n_\\mathrm{hidden})$ [$b^\\mathrm{h} = (n_\\mathrm{hidden},)$],\n",
    "we can easily feed the network all our training data in one go by using matrix products.\n",
    "\n",
    "The final output should be the probabilities $\\hat{y}^L$ of dimension $(n_\\mathrm{data}, n_\\mathrm{categories})$.\n",
    "\n",
    "Moreover, in order to implement the backpropagation step we also need the output signals ($\\hat{a}^h$) from the hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** that weights and biases are global variables in this implementation (not good coding practice!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement the feed-forward pass\n",
    "* Implement the feed-forward pass by modifying the code below.\n",
    "* Test the implementation by feeding the input data and making predictions for all class labels (note that the network parameters have not been optimized so the predictions will be rubbish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the feed-forward pass, subscript h = hidden layer\n",
    "#\n",
    "# Note that weights and biases are global variables in this implementation\n",
    "# (not good coding practice!)\n",
    "\n",
    "def feed_forward(X):\n",
    "    \"\"\"\n",
    "    Feed-forward pass.\n",
    "    Uses hidden_weights, hidden_bias, output_weights, output_bias\n",
    "    \n",
    "    Args:\n",
    "        X (array[float]): input to the neural network\n",
    "\n",
    "    Returns:\n",
    "        a_h (array[float]): activation in the hidden layer\n",
    "        probabilities (array[float]): probabilities of each category\n",
    "    \"\"\"\n",
    "    # Modify the code below\n",
    "    #\n",
    "    a_h = None\n",
    "    probabilities = None\n",
    "    \n",
    "    return a_h, probabilities\n",
    "\n",
    "# prediction of class label\n",
    "def predict(X):\n",
    "    \"\"\"\n",
    "    Return a prediction by finding the class with the highest likelihood.\n",
    "\n",
    "    Args:\n",
    "        X (array[float]): input to the neural network\n",
    "\n",
    "    Returns:\n",
    "        label (integer): index of the category with the highest probability\n",
    "    \"\"\"\n",
    "\n",
    "    a_h, probabilities = feed_forward(X)\n",
    "    \n",
    "    # Modify the code below\n",
    "    #\n",
    "    class_label = None\n",
    "    return class_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose cost function and optimizer\n",
    "\n",
    "To measure how well our neural network is doing we need to introduce a cost function.  \n",
    "We will call the function that gives the error of a single sample output the *loss* function, and the function\n",
    "that gives the total error of our network across all samples the *cost* function.\n",
    "A typical choice for multiclass classification is the *cross-entropy* loss.  \n",
    "\n",
    "In *multiclass* classification it is common to treat each integer label as a so called *one-hot* vector:  \n",
    "\n",
    "$$ t = 5 \\quad \\rightarrow \\quad \\hat{y} = (0, 0, 0, 0, 0, 1, 0, 0, 0, 0) ,$$  \n",
    "\n",
    "$$ t = 1 \\quad \\rightarrow \\quad \\hat{y} = (0, 1, 0, 0, 0, 0, 0, 0, 0, 0) ,$$  \n",
    "\n",
    "i.e. a binary bit string of length $C$, where $C = 10$ is the number of classes.  \n",
    "\n",
    "Let $t_{ic}$ denote the $c$-th component of the $i$-th one-hot vector.  \n",
    "We define the cost function $\\mathcal{C}$ as a sum over the cross-entropy loss for each point $\\hat{x}_i$ in the dataset.\n",
    "\n",
    "In the one-hot representation only one of the terms in the loss function is non-zero, namely the\n",
    "probability of the correct category $c'$  \n",
    "(i.e. the category $c'$ such that $t_{ic'} = 1$). This means that the cross entropy loss only punishes you for how wrong\n",
    "you got the correct label. The probability of category $c$ is given by the softmax function. The vector $\\hat{\\theta}$ represents the parameters of our network, i.e. all the weights and biases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to categorical turns our integer vector into a onehot representation\n",
    "# we implement it in in numpy\n",
    "def to_categorical_numpy(integer_vector):\n",
    "    n_inputs = len(integer_vector)\n",
    "    n_categories = np.max(integer_vector) + 1\n",
    "    onehot_vector = np.zeros((n_inputs, n_categories))\n",
    "    onehot_vector[range(n_inputs), integer_vector] = 1\n",
    "    \n",
    "    return onehot_vector\n",
    "\n",
    "Y_train_onehot, Y_test_onehot = to_categorical_numpy(Y_train), to_categorical_numpy(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the cost function\n",
    "\n",
    "The network is trained by finding the weights and biases that minimize the cost function. One of the most widely used classes of methods is *gradient descent* and its generalizations. The idea behind gradient descent\n",
    "is simply to adjust the weights in the direction where the gradient of the cost function is large and negative. This ensures we flow toward a *local* minimum of the cost function.  \n",
    "Each parameter $\\theta$ is iteratively adjusted according to the rule  \n",
    "\n",
    "$$ \\theta_{i+1} = \\theta_i - \\eta \\nabla \\mathcal{C}(\\theta_i) ,$$\n",
    "\n",
    "where $\\eta$ is known as the *learning rate*, which controls how big a step we take towards the minimum.  \n",
    "This update can be repeated for any number of iterations, or until we are satisfied with the result.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra feature (not necessarily used in this implementation)\n",
    "A simple and effective improvement is a variant called *Batch Gradient Descent*.  \n",
    "Instead of calculating the gradient on the whole dataset, we calculate an approximation of the gradient\n",
    "on a subset of the data called a *minibatch*.  \n",
    "If there are $N$ data points and we have a minibatch size of $M$, the total number of batches\n",
    "is $N/M$.  \n",
    "We denote each minibatch $B_k$, with $k = 1, 2,...,N/M$. The gradient then becomes:  \n",
    "\n",
    "$$ \\nabla \\mathcal{C}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\nabla \\mathcal{L}_i(\\theta) \\quad \\rightarrow \\quad\n",
    "\\frac{1}{M} \\sum_{i \\in B_k} \\nabla \\mathcal{L}_i(\\theta) ,$$\n",
    "\n",
    "i.e. instead of averaging the loss over the entire dataset, we average over a minibatch.  \n",
    "\n",
    "This has two important benefits:  \n",
    "1. Introducing stochasticity decreases the chance that the algorithm becomes stuck in a local minima.  \n",
    "\n",
    "2. It significantly speeds up the calculation, since we do not have to use the entire dataset to calculate the gradient.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "It is common to add an extra term to the cost function, proportional\n",
    "to the size of the weights.  This is equivalent to constraining the\n",
    "size of the weights, so that they do not grow out of control.\n",
    "Constraining the size of the weights means that the weights cannot\n",
    "grow arbitrarily large to fit the training data, and in this way\n",
    "reduces *overfitting*.\n",
    "\n",
    "We will measure the size of the weights using the so called *L2-norm*, meaning our cost function becomes:  \n",
    "\n",
    "$$  \\mathcal{C}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}_i(\\theta) \\quad \\rightarrow \\quad\n",
    "\\frac{1}{N} \\sum_{i=1}^N  \\mathcal{L}_i(\\theta) + \\alpha \\frac{1}{2}  \\lvert \\lvert \\hat{w} \\rvert \\rvert_2^2 \n",
    "= \\frac{1}{N} \\sum_{i=1}^N  \\mathcal{L}(\\theta) + \\alpha \\frac{1}{2} \\sum_{ij} w_{ij}^2,$$  \n",
    "\n",
    "i.e. we sum up all the weights squared. The factor $\\alpha$ is known as the weight decay and it is a regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-propagation\n",
    "In order to train the model, we need to calculate the derivative of\n",
    "the cost function with respect to every bias and weight in the\n",
    "network.  In total our network has $(64 + 1)\\times 50=3250$ parameters in\n",
    "the hidden layer and $(50 + 1)\\times 10=510$ parameters in the output\n",
    "layer (the $+1$ is for the bias), and the gradient must be calculated for\n",
    "every parameter.  We use the *backpropagation* algorithm, which is a clever use of the chain rule that allows us to\n",
    "calculate the gradient efficently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix  multiplication\n",
    "\n",
    "To more efficently train our network these equations are implemented using matrix operations.  \n",
    "The error in the output layer is calculated simply as, with $\\hat{t}$ being our targets,  \n",
    "\n",
    "$$ \\delta_L = \\hat{t} - \\hat{y} \\qquad (n_\\mathrm{data}, n_\\mathrm{categories}) .$$  \n",
    "\n",
    "The gradient for the output weights is calculated as  \n",
    "\n",
    "$$ \\nabla W_{L} = \\hat{a}^T \\delta_L  \\qquad (n_\\mathrm{hidden}, n_\\mathrm{categories}) ,$$\n",
    "\n",
    "where $\\hat{a} = (n_\\mathrm{data}, n_\\mathrm{hidden})$. This simply means that we are summing up the gradients for each input.  \n",
    "Since we are going backwards we have to transpose the activation matrix.  \n",
    "\n",
    "The gradient with respect to the output bias is then  \n",
    "\n",
    "$$ \\nabla \\hat{b}_{L} = \\sum_{i=1}^{n_\\mathrm{data}} \\delta_L \\qquad (n_\\mathrm{categories}) .$$  \n",
    "\n",
    "The error in the hidden layer is  \n",
    "\n",
    "$$ \\Delta_h = \\delta_L W_{L}^T \\circ f'(z_{h}) = \\delta_L W_{L}^T \\circ a_{h} \\circ (1 - a_{h}) \\qquad (n_\\mathrm{data}, n_\\mathrm{hidden}) ,$$  \n",
    "\n",
    "where $f'(a_{h})$ is the derivative of the activation in the hidden layer. The matrix products mean\n",
    "that we are summing up the products for each neuron in the output layer. The symbol $\\circ$ denotes\n",
    "the *Hadamard product*, meaning element-wise multiplication.  \n",
    "\n",
    "This again gives us the gradients in the hidden layer:  \n",
    "\n",
    "$$ \\nabla W_{h} = X^T \\delta_h \\qquad (n_\\mathrm{features}, n_\\mathrm{hidden}) ,$$  \n",
    "\n",
    "$$ \\nabla b_{h} = \\sum_{i=1}^{n_\\mathrm{data}} \\delta_h \\qquad (n_\\mathrm{hidden}) .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The back-propagation is implemented below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def backpropagation(X, Y):\n",
    "    a_h, probabilities = feed_forward(X)\n",
    "    \n",
    "    # error in the output layer\n",
    "    error_output = probabilities - Y\n",
    "    # error in the hidden layer\n",
    "    error_hidden = np.matmul(error_output, output_weights.T) * a_h * (1 - a_h)\n",
    "    \n",
    "    # gradients for the output layer\n",
    "    output_weights_gradient = np.matmul(a_h.T, error_output)\n",
    "    output_bias_gradient = np.sum(error_output, axis=0)\n",
    "    \n",
    "    # gradient for the hidden layer\n",
    "    hidden_weights_gradient = np.matmul(X.T, error_hidden)\n",
    "    hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "    return output_weights_gradient, output_bias_gradient, hidden_weights_gradient, hidden_bias_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Train the network\n",
    "* Implement the training of the network by modifying the code below.\n",
    "* Use a fixed learning rate $\\eta=0.001$ and a weight decay $\\alpha=0.1$, but feel free to explore other parameters.\n",
    "* Make 1000 iterations, where you employ the entire training set in each iteration.\n",
    "* What is the accuracy on the training set before and after the training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is as you would expect just the number of images correctly labeled divided by the total number of images. A perfect classifier will have an accuracy score of $1$.  \n",
    "\n",
    "$$ \\text{Accuracy} = \\frac{\\sum_{i=1}^n I(\\hat{y}_i = y_i)}{n} ,$$  \n",
    "\n",
    "where $I$ is the indicator function, $1$ if $\\hat{y}_i = y_i$ and $0$ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 0.046\n"
     ]
    }
   ],
   "source": [
    "# accuracy score from scikit library\n",
    "print(f\"Accuracy on training data: {accuracy_score(predict(X_train), Y_train):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.001\n",
    "alpha = 0.1\n",
    "iters = 1000\n",
    "for i in range(iters):\n",
    "    # calculate gradients\n",
    "    dWo, dBo, dWh, dBh = backpropagation(X_train, Y_train_onehot)\n",
    "    \n",
    "    # Add the regularization term gradients to the weights\n",
    "    # Modify the code below\n",
    "    #\n",
    "    dWo += 0\n",
    "    dWh += 0\n",
    "    \n",
    "    # Finally, update weights and biases\n",
    "    # Modify the code below\n",
    "    #\n",
    "    output_weights -= 0\n",
    "    output_bias -= 0\n",
    "    hidden_weights -= 0\n",
    "    hidden_bias -= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Evaluate model performance on test data\n",
    "To measure the performance of our network we evaluate how well it does it data it has never seen before, i.e. the test data.  \n",
    "We measure the performance of the network using the *accuracy* score.  \n",
    "* What is the accuracy score on the test data?\n",
    "* Take a look at (some of) the images that were incorrectly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Study the neural network before, after and during training\n",
    "* Plot the mean of the absolute values of gradients with respect to the weights in the hidden layer, and the variance of those gradients during training. Do you see the convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
