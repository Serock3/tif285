{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3: Extra problems\n",
    "## Learning from data [TIF285], Chalmers, Fall 2019\n",
    "\n",
    "Last revised: 17-Oct-2019 by Christian Forssén [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See deadline on the course web page\n",
    "- This problem set is performed individually (but collaboration is encouraged) and contains a number of basic and extra problems; you can choose which and how many to work on.\n",
    "- See examination rules on the course web page.\n",
    "- Hand-in is performed through the following **two** actions:\n",
    "  - Upload of your solution in the form of a jupyter notebook, or python code, via Canvas.\n",
    "  - Answer the corresponding questions on OpenTA.\n",
    "  \n",
    "  Note that the hand-in is not complete, and will not be graded, if any of those actions is not performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill your personal details\n",
    "- Name: **Holmin, Sebastian**\n",
    "- Personnummer: **970602-3679**\n",
    "  <br/>\n",
    "  (civic registration number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Problems\n",
    "The 8 extra points of this problem set are distributed over two problems:\n",
    "5. Bayesian Optimization (4 extra points)\n",
    "6. Deep neural network python class (4 extra points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Problem 6: (extra) Deep neural network python class\n",
    "### (4 extra points)\n",
    "<!-- Author: -->  \n",
    "Christian Forssén, Chalmers, and\n",
    "Morten Hjorth-Jensen, University of Oslo and Michigan State University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a code for doing neural networks with back propagation\n",
    "\n",
    "One can identify a set of key steps when using neural networks to solve supervised learning problems:  \n",
    "  \n",
    "1. Collect and pre-process data  \n",
    "1. Define model and architecture  \n",
    "1. Choose cost function and optimizer  \n",
    "1. Train the model  \n",
    "1. Adjust hyperparameters (if necessary, network architecture)\n",
    "1. Evaluate model performance on test data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect and pre-process data\n",
    "\n",
    "Here we will be using the MNIST dataset, which is readily available through the **scikit-learn**\n",
    "package. You may also find it for example [here](http://yann.lecun.com/exdb/mnist/).  \n",
    "The *MNIST* (Modified National Institute of Standards and Technology) database is a large database\n",
    "of handwritten digits that is commonly used for training various image processing systems.  \n",
    "The MNIST dataset consists of 70 000 images of size $28\\times 28$ pixels, each labeled from 0 to 9.  \n",
    "The scikit-learn dataset we will use consists of a selection of 1797 images of size $8\\times 8$ collected and processed from this database.  \n",
    "\n",
    "Since each input image is a 2D matrix, we need to flatten the image\n",
    "(i.e. \"unravel\" the 2D matrix into a 1D array) to turn the data into a\n",
    "design/feature matrix. This means we lose all spatial information in the\n",
    "image, such as locality and translational invariance. More complicated\n",
    "architectures such as Convolutional Neural Networks can take advantage\n",
    "of such information, and are most commonly applied when analyzing\n",
    "images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: (n_data, pixel_width, pixel_height) = (1797, 8, 8)\n",
      "                       with labels (n_data) = (1797,)\n",
      "\n",
      "flattened input, X: (n_inputs, n_features)  = (1797, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import \n",
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "# download MNIST dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# define inputs and labels\n",
    "inputs = digits.images\n",
    "labels = digits.target\n",
    "\n",
    "print(f\"inputs: (n_data, pixel_width, pixel_height) = {inputs.shape}\")\n",
    "print(f\"                       with labels (n_data) = {labels.shape}\")\n",
    "\n",
    "\n",
    "# flatten the image\n",
    "# the value -1 means dimension is inferred from the remaining dimensions: 8x8 = 64\n",
    "n_inputs = len(inputs)\n",
    "inputs = inputs.reshape(n_inputs, -1)\n",
    "print(f\"\\nflattened input, X: (n_inputs, n_features)  = {inputs.shape}\")\n",
    "\n",
    "\n",
    "# choose some random images to display\n",
    "indices = np.arange(n_inputs)\n",
    "random_indices = np.random.choice(indices, size=5)\n",
    "\n",
    "for i, image in enumerate(digits.images[random_indices]):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(f\"Label: {digits.target[random_indices[i]]:1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test datasets\n",
    "\n",
    "We will reserve $70 \\%$ of our dataset for training and $30 \\%$ for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 1257\n",
      "Number of test images:      540\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "train_size = 0.7\n",
    "test_size = 1 - train_size\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(inputs, labels, train_size=train_size,\n",
    "                                                    test_size=test_size)\n",
    "\n",
    "print(f\"Number of training images: {len(X_train):4}\")\n",
    "print(f\"Number of test images:     {len(X_test):4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to categorical turns our integer vector into a onehot representation\n",
    "# we implement it in in numpy\n",
    "def to_categorical_numpy(integer_vector):\n",
    "    n_inputs = len(integer_vector)\n",
    "    n_categories = np.max(integer_vector) + 1\n",
    "    onehot_vector = np.zeros((n_inputs, n_categories))\n",
    "    onehot_vector[range(n_inputs), integer_vector] = 1\n",
    "    \n",
    "    return onehot_vector\n",
    "\n",
    "Y_train_onehot, Y_test_onehot = to_categorical_numpy(Y_train), to_categorical_numpy(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Full object-oriented implementation\n",
    "\n",
    "It is very natural to think of the network as an object, with specific instances of the network\n",
    "being realizations of this object with different hyperparameters. An implementation using Python classes provides a clean structure and interface. \n",
    "\n",
    "The cost function should contain a $\\lambda \\sum_i \\frac{1}{2} w_i^2$ regularizer. (*Note* that `lmbd` is used here for the decay width in the regularizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skeleton for a Python class NeuralNetwork is implemented below. Specifically your tasks are to:\n",
    "* Implement the `feed_forward` method of the `NeuralNetwork` class. It should update `self.z_h`, `self.a_h`, `self.z_o`, and `self.probabilities`.\n",
    "* Correct the `backpropagation` method of the `NeuralNetwork` class. You must add the weight gradients from the regularizer term.\n",
    "* Implement the `predict` method of the `NeuralNetwork` class. It should return the class label (integer).\n",
    "* Implement the `predict_probabilities` method of the `NeuralNetwork` class. It should return the array of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(\n",
    "            self,\n",
    "            X_data,\n",
    "            Y_data,\n",
    "            n_hidden_neurons=50,\n",
    "            n_categories=10,\n",
    "            epochs=10,\n",
    "            batch_size=100,\n",
    "            eta=0.1,\n",
    "            lmbd=0.0):\n",
    "\n",
    "        self.X_data_full = X_data\n",
    "        self.Y_data_full = Y_data\n",
    "\n",
    "        self.n_inputs = X_data.shape[0]\n",
    "        self.n_features = X_data.shape[1]\n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.n_categories = n_categories\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.iterations = self.n_inputs // self.batch_size\n",
    "        self.eta = eta\n",
    "        self.lmbd = lmbd\n",
    "        \n",
    "        self.create_biases_and_weights()\n",
    "        self.z_h, self.a_h, self.z_o, self.probabilities = None,None,None,None\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1./(1. + np.exp(-z))\n",
    "\n",
    "    def softmax(self, z):\n",
    "        return np.exp(z)/np.sum(np.exp(z),axis=1).reshape(-1,1)\n",
    "\n",
    "    def create_biases_and_weights(self):\n",
    "        self.hidden_weights = np.random.randn(self.n_features, self.n_hidden_neurons)\n",
    "        self.hidden_bias = np.zeros(self.n_hidden_neurons) + 0.01\n",
    "\n",
    "        self.output_weights = np.random.randn(self.n_hidden_neurons, self.n_categories)\n",
    "        self.output_bias = np.zeros(self.n_categories) + 0.01\n",
    "\n",
    "    def feed_forward(self):\n",
    "        # feed-forward for training\n",
    "        self.z_h = self.X_data.dot(self.hidden_weights)+self.hidden_bias\n",
    "        self.a_h = self.sigmoid(self.z_h)\n",
    "        self.z_o =self.a_h.dot(self.output_weights)+self.output_bias\n",
    "        self.probabilities = self.softmax(self.z_o)\n",
    "\n",
    "    def backpropagation(self):        \n",
    "        error_output = self.probabilities - self.Y_data\n",
    "        error_hidden = np.matmul(error_output, self.output_weights.T) * self.a_h * (1 - self.a_h)\n",
    "\n",
    "        self.output_weights_gradient = np.matmul(self.a_h.T, error_output)\n",
    "        self.output_bias_gradient = np.sum(error_output, axis=0)\n",
    "\n",
    "        self.hidden_weights_gradient = np.matmul(self.X_data.T, error_hidden)\n",
    "        self.hidden_bias_gradient = np.sum(error_hidden, axis=0)\n",
    "\n",
    "        # Add the weight gradients from the regularizer term.\n",
    "        # Add code here\n",
    "        self.output_weights_gradient  += self.lmbd*self.output_weights\n",
    "        self.hidden_weights_gradient += self.lmbd*self.hidden_weights\n",
    "\n",
    "        \n",
    "        self.output_weights -= self.eta * self.output_weights_gradient\n",
    "        self.output_bias -= self.eta * self.output_bias_gradient\n",
    "        self.hidden_weights -= self.eta * self.hidden_weights_gradient\n",
    "        self.hidden_bias -= self.eta * self.hidden_bias_gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.X_data = X\n",
    "        self.feed_forward()\n",
    "        return np.argmax(self.probabilities,axis=1)\n",
    "\n",
    "    def predict_probabilities(self, X):\n",
    "        self.X_data = X\n",
    "        self.feed_forward()\n",
    "        raise self.probabilities\n",
    "\n",
    "    def train(self):\n",
    "        data_indices = np.arange(self.n_inputs)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for j in range(self.iterations):\n",
    "                # pick datapoints with replacement\n",
    "                chosen_datapoints = np.random.choice(\n",
    "                    data_indices, size=self.batch_size, replace=False\n",
    "                )\n",
    "\n",
    "                # minibatch training data\n",
    "                self.X_data = self.X_data_full[chosen_datapoints]\n",
    "                self.Y_data = self.Y_data_full[chosen_datapoints]\n",
    "\n",
    "                self.feed_forward()\n",
    "                self.backpropagation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Train and evaluate model performance on test data\n",
    "\n",
    "We measure the performance of the network using the *accuracy* score. Initialize the neural network as specified below and:\n",
    "* Measure the accuracy on the training data before training.\n",
    "* Measure the accuracy on the training data after training.\n",
    "* Measure the accuracy on the test data after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data before training: 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Anaconda\\envs\\tif285-env\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data after training: 0.106\n",
      "Accuracy on training test after training: 0.089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "n_hidden_neurons = 50\n",
    "n_categories = 10\n",
    "\n",
    "eta=0.1\n",
    "lmbd=0.1\n",
    "\n",
    "# ensure the same random numbers appear every time\n",
    "np.random.seed(0)\n",
    "\n",
    "dnn = NeuralNetwork(X_train, Y_train_onehot, eta=eta, lmbd=lmbd, epochs=epochs, batch_size=batch_size,\n",
    "                    n_hidden_neurons=n_hidden_neurons, n_categories=n_categories)\n",
    "\n",
    "print(f\"Accuracy on training data before training: {accuracy_score(dnn.predict(X_train), Y_train):.3f}\")\n",
    "dnn.train()\n",
    "print(f\"Accuracy on training data after training: {accuracy_score(dnn.predict(X_train), Y_train):.3f}\")\n",
    "print(f\"Accuracy on training test after training: {accuracy_score(dnn.predict(X_test), Y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Improving performance\n",
    "\n",
    "As you might see the network does not seem to be learning at all. It seems to be just guessing the label for each image.  \n",
    "In order to obtain a network that does something useful, we will have to tune the *hyperparameters* such as learning rate and regularization parameter. Those are hugely influential for the performance of the network. \n",
    "\n",
    "Typically a *grid-search* is performed, wherein we test different hyperparameters separated by orders of magnitude. For example we could test the learning rates $\\eta = 10^{-5}, 10^{-4},...,10^{1}$ with different regularization parameters $\\lambda = 10^{-5}, 10^{-4},,...,10^{1}$.  \n",
    "\n",
    "If this does not improve network performance, you may want to consider altering the network architecture, adding more neurons or hidden layers.  \n",
    "Andrew Ng goes through some of these considerations in this [video](https://youtu.be/F1ka6a13S9I). You can find a summary of the video [here](https://kevinzakka.github.io/2016/09/26/applying-deep-learning/).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perform a grid search to find the optimal hyperparameters for the network. \n",
    "* What model gives you the best performance?\n",
    "* What is the best accuracy on the test set?\n",
    "Note that we are only using 1 layer with 50 neurons, and human performance is estimated to be around $98\\%$ ($2\\%$ error rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\programs\\Anaconda\\envs\\tif285-env\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: overflow encountered in exp\n",
      "D:\\programs\\Anaconda\\envs\\tif285-env\\lib\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: overflow encountered in exp\n",
      "D:\\programs\\Anaconda\\envs\\tif285-env\\lib\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best eta  0.01\n",
      "Best lambda  0.1\n",
      "Best accuracy on training test after training: 0.967\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "n_hidden_neurons = 50\n",
    "n_categories = 10\n",
    "\n",
    "etas = [10**n for n in range(-5,2)]\n",
    "lmbdas = [10**n for n in range(-5,2)]\n",
    "\n",
    "result_t_a = np.zeros((7,7)) #Store results from training data after training\n",
    "\n",
    "for i, eta in enumerate(etas):\n",
    "    for j, lmbd in enumerate(lmbdas):\n",
    "        np.random.seed(0) #Ensure identical training for all parameters\n",
    "\n",
    "        dnn = NeuralNetwork(X_train, Y_train_onehot, eta=eta, lmbd=lmbd, epochs=epochs, batch_size=batch_size,\n",
    "                            n_hidden_neurons=n_hidden_neurons, n_categories=n_categories)\n",
    "        dnn.train()\n",
    "        result_t_a[i][j] = accuracy_score(dnn.predict(X_test), Y_test)\n",
    "        \n",
    "ind = np.unravel_index(np.argmax(result_t_a, axis=None), result_t_a.shape)\n",
    "print('Best eta ', etas[ind[0]])\n",
    "print('Best lambda ', lmbdas[ind[1]])\n",
    "print(f\"Best accuracy on training test after training: {result_t_a[ind]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
