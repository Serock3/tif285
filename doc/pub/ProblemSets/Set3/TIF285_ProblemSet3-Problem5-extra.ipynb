{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3: Extra problems\n",
    "## Learning from data [TIF285], Chalmers, Fall 2019\n",
    "\n",
    "Last revised: 11-Oct-2019 by Christian Forss√©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See deadline on the course web page\n",
    "- This problem set is performed individually (but collaboration is encouraged) and contains a number of basic and extra problems; you can choose which and how many to work on.\n",
    "- See examination rules on the course web page.\n",
    "- Hand-in is performed through the following **two** actions:\n",
    "  - Upload of your solution in the form of a jupyter notebook, or python code, via Canvas.\n",
    "  - Answer the corresponding questions on OpenTA.\n",
    "  \n",
    "  Note that the hand-in is not complete, and will not be graded, if any of those actions is not performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill your personal details\n",
    "- Name: **Lastname, Firstname**\n",
    "- Personnummer: **YYMMDD-XXXX**\n",
    "  <br/>\n",
    "  (civic registration number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Problems\n",
    "The 8 extra points of this problem set are distributed over two problems:\n",
    "5. Bayesian Optimization (4 extra points)\n",
    "6. Deep neural network python class (4 extra points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Problem 5: (extra) Bayesian Optimization\n",
    "### (4 extra points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEYS\n",
    "#\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "import GPy\n",
    "\n",
    "# Not really needed, but nicer plots\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A univariate minimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to minimize the function\n",
    "$$\n",
    "f(x) = \\sin(6 x) + 0.2 x^2 - 0.7 x\n",
    "$$\n",
    "on the interval $x \\in [-5,5]$.\n",
    "\n",
    "The aim is to find the position of the minimum $x^*$ to within $\\pm 0.05$ under the constraint that we would like to make as few function evaluations as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Using \"standard\" optimization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. **Plot the true function and indicate the position of the minimum**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Find the minimum using `scipy.optimize.minimize` with `method='Nelder-Mead'`. \n",
    "* Choose the starting point randomly from a uniform pdf $U(-5,5)$. \n",
    "* Repeat one hundred times. **Do you always get the same minimum?**\n",
    "* More specifically, set the tolerance of the optimizer to `tol=0.01` and check for success by the criterion $|x^* - x^*_\\mathrm{opt}| < 0.05$, where $x^*_\\mathrm{opt}$ is the result from the optimizer.\n",
    "* Be quantitative about the average number of function evaluations that are needed per successful optimization. Compute the ratio of the total number of function evaluations number (summed over the 100 tries with different starting points) with the number of successful attempts.  \n",
    "  *Hint*: The number of function evaluations from a `scipy.optimize.minimize` result is returned in the ``OptimizeResult`` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Build your own BayesOpt algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now try a very different approach to minimize an objective function (this is a conventional name for the function that we are trying to minimize or maximize). The approach is known as **Bayesian optimization** and the basic idea is the following:\n",
    "* Sample a few random points and build a **statistical model** for the unknown function (usually a Gaussian Process model).\n",
    "* Make a decision which point to sample next based on a so called **acquisition function** evaluated from the statistical model. This will incorporate our current knowledge about the function including our uncertainty for its value in different regions.\n",
    "* Improve the statistical model using the new sample. Continue sampling new points according to the acquisition function.\n",
    "* If done correctly, this approach will balance **exploration** of new regions that might contain the minimum, and **exploitation** of searching the region that is currently most promising.\n",
    "* Very importantly, this method also works when you are dealing with **noisy objective functions**, i.e. when your \"measurement\" of its value at a new point in parameter space contains some random noise.\n",
    "\n",
    "Your task is to repeat the above minimization with **your own Bayesian Optimization algorithm**, that should be assembled as described below. Such algoritms are built into libraries such as `Scikit-optimize` and `GPyOpt`, but we will build our own simple version using functions from `numpy`, `scipy`, and `GPy` (for building the statistical model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudo-code for BayesOpt is the following (see specific hints for your implementation at the end):\n",
    "1. pick starting points $\\mathbf{x}^{(1)},\\mathbf{x}^{(2)},\\ldots \\mathbf{x}^{(k)}$, where $k \\geq 2$\n",
    "1. evaluate the objective function $f(\\mathbf{x})$ to obtain $y^{(i)}=f(\\mathbf{x}^{(i)})$ for $i=1,\\ldots,k$\n",
    "1. initialize a data vector $\\mathcal{D}_k = \\left\\{(\\mathbf{x}^{(i)},y^{(i)})\\right\\}_{i=1}^k$\n",
    "1. select a statistical model for $f(\\mathbf{x})$\n",
    "1. **For** {$n=k+1,k+2,\\ldots$}\n",
    "   1.    select $\\mathbf{x}^{(n)}$ by optimizing the acquisition function: $\\mathbf{x}^{(n)} = \\underset{\\mathbf{x}}{\\text{arg max}}\\, \\mathcal{A}(\\mathbf{x}|\\mathcal{D}_{n-1})$\n",
    "   1.    evaluate the objective function to obtain $y^{(n)}=f(\\mathbf{x}^{(n)})$\n",
    "   1.    augment the data vector $\\mathcal{D}_n = \\left\\{\\mathcal{D}_{n-1} , (\\mathbf{x}^{(n)},y^{(n)})\\right\\}$\n",
    "   1.    update the statistical model for $f(\\mathbf{x})$\n",
    "1. **end for**\n",
    "\n",
    "   Check for the minimum in the data vector that has been collected (note that it doesn't necessarily have to be the last sample).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hints:**\n",
    "* You have to implement all steps in the above pseudo-code.\n",
    "* You can try with $k=2$ starting points.\n",
    "* For the statistical model you can use `GPyOpt`. Follow the examples from the lectures and the exercise notebook.\n",
    "* Any knowledge about the objective function should be built into the covariance function. Let us assume that we dan't have much information and that we use a standard RBF kernel.\n",
    "* It is recommended to constrain the RBF lengthscale so that it doesn't become unrealistic small. With the GPy model called `model`, such a constraint can be imposed using `model['rbf.lengthscale'].constrain_bounded(.1,10)`.\n",
    "* Implement the so called Lower Confidence Bound (LCB) acquisition function for use in step 5A. Then, the acquisition function is simply: $\\mathcal{A}(\\boldsymbol{x}; | \\mathcal{D}_{n-1}) = -\\mu(\\boldsymbol{x}) + \\sigma(\\boldsymbol{x})$, where\n",
    "  * $\\mu(\\boldsymbol{x})$ is the mean of the GP model trained with the data $\\mathcal{D}_{n-1})$.\n",
    "  * $\\sigma(\\boldsymbol{x})$ is the standard deviation of the GP model trained with the data $\\mathcal{D}_{n-1})$.\n",
    "* Remember that the statistical model has to be updated (the hyperparameters re-optimized) at step 5D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "* Implement the BayesOpt minimizer\n",
    "* Assume that you are allowed a total of 100 function evaluations ($k$ of them for the starting points and $k-2$ in the loop). Are you able to find the minimum to within $\\pm 0.05$?\n",
    "* Plot the final statistical model together with the true function and mark the minimum.\n",
    "* Plot also the convergence of the minimum value $\\min(y_n)$ as a function of the iteration number $n \\in \\{1, \\ldots, 100\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
