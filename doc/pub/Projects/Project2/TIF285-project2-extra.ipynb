{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project II: Machine Learning the Ising Phase Transition\n",
    "## Learning from data [TIF285], Chalmers, Fall 2019\n",
    "\n",
    "#### Christian Forssén and Shahnawaz Ahmed, Chalmers\n",
    "Last revised: 16-Oct-2019 by Christian Forssén [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See deadline on the course web page\n",
    "- This project is performed in groups of two students. \n",
    "- The second part of the project is optional and only gives extra points. See examination rules on the course web page.\n",
    "- Hand-in your written report via Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written report\n",
    "- Page limit: 6 pages (excluding title page and list of references). 3 extra pages are allowed when doing also the optional extra task.\n",
    "- Give a short description of the nature of the problem and the methods you have used.\n",
    "- Include your results either in figure form or in a table. All tables and figures should have relevant captions and labels on the axes.\n",
    "- Try to give an interpretation of you results.\n",
    "- Upload the source code of your program as a separate file (.ipynb or .py). Comment your program properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project background and main tasks\n",
    "\n",
    "See the `TIF285-project2.ipynb` notebook for the project background and the description of the main tasks. This notebook just contains a description of the extra tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Understanding the neural network (extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have implemented three different strategies based on Neural Networks to identify phase transitions from raw data. But what do the networks learn and what can they tell us about phase transitions? Do the networks understand the concept of magnetization or critical temperature? Or they are just memorizing the data and an input-output relationship?\n",
    "\n",
    "* In this task you will try to reproduce the results from the toy-model in the Supplementary section of the paper Carrasquilla, Juan, and Roger G. Melko. ``Machine learning phases of matter.'' [Nature Physics (2017) 13, 431–434](https://www.nature.com/articles/nphys4035).\n",
    "\n",
    "* First, read section 1 in the [supplemenart section](https://media.nature.com/original/nature-assets/nphys/journal/v13/n5/extref/nphys4035-s1.pdf) to understand the toy model and how the authors have parameterized the weights such that the result of the first hidden layer network is a function of the magnetization, i.e, $Wx + b = \\frac{1}{1+\\epsilon}\\{m(x) - \\epsilon, -m(x) - \\epsilon, m(x) + \\epsilon\\}$. The design is such that the first two perceptrons activate when the input states are mostly polarized, while the third one activate if the states are polarized up or unpolarized.\n",
    "\n",
    "* Your task is to show that in a single-hidden layered network implemented to classify phases, the intermediate result $Wx + b$ follows the toy model proposed in the paper, i.e., the intermediate output as a function of magnetization give three different straight lines with their slopes being $\\pm \\alpha$ where $\\alpha$ is some arbitrary number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-tasks\n",
    "\n",
    "* Train a neural network with three neurons in the hidden layer (using a sigmoid activation function) and plot the intermediate output as a function of the magnetization. \n",
    "\n",
    "  [*Hint*: in order to get a plot similar to the paper, you might need to make your input symmetric by encoding the spins as +1 and -1]\n",
    "\n",
    "\n",
    "* Fit straight lines to your plots to show that your trained network approximately fits the proposed toy model in the paper, i.e., that you get straight lines with approximately the following behaviour $Wx + b = \\frac{1}{1+\\epsilon}\\{m(x) - \\epsilon, -m(x) - \\epsilon, m(x) + \\epsilon\\}$\n",
    "  \n",
    "\n",
    "* Now train a network with **10 neurons** in the hidden layer and show that there is a redundancy in the number of neurons by plotting the $Wx + b$ for all of them to show that most neurons behave in one of four different ways.\n",
    "\n",
    "\n",
    "* *Bonus*: Comment on why the intermediate outputs from a **10 neuron network** gets grouped into four classes instead of the three as per the toy-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: A simple Bayesian neural network (extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a bayesian binary classifier that can take $(E,|m|)$ as input data and predict a binary label (0=below $T_c$, 1=above $T_c$). \n",
    "* Use only high- and low-tempterature data for training (so that predictions for intermediate temperature data should be more difficult). Use normalized data as in Task 1.\n",
    "* The weights (and bias) of the single neuron binary classifier will be described by pdf:s that we will sample from using MCMC.\n",
    "* Use a Gaussian prior on the two weights and the bias (with ``weight decay'' $\\sigma = 1.0)$.\n",
    "* Construct the (log) likelihood as in logistic regression (i.e. as used in Task 1). \n",
    "* Use, e.g., `emcee`, for the MCMC sampling.\n",
    "* The prediction for a given input should be characterized by a pdf; i.e. the predicted probability for the state belonging to class 1 (above $T_c$) will itself be described by a pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-tasks\n",
    "(a) Plot the pdf:s for the weights and the bias of your Bayesian binary classifier.\n",
    "\n",
    "(b) Study in particular the **prediction** of your Bayesian binary classifier for inputs $(E,|m|)$ that corresponds to:\n",
    "1. a low-temperature configuration.\n",
    "1. a high-temperature configuration.\n",
    "1. a temperature very close to the critical one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
